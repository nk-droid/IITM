{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The diagram below shows a neural network used for a classification problem. The network contains two hidden layers and one output layer. The input to the network is a column vector $x \\in \\R^{3}$. The first hidden layer contains 3 neurons, the second hidden layer contains 3 neurons and the output layer contains 3 neurons. Each neuron in the $l^{th}$ layer is connected to all the neurons in the $(l+1)^{th}$ layer. Each neuron has a bias connected to it (not explicitly shown in the figure).\n",
    "\n",
    "<img src=\"https://backend.seek.onlinedegree.iitm.ac.in/22t3_cs3004/assets/img/W3GA1.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights and biases are initialized as given below...\n",
    "\n",
    "$W_1=\\begin{bmatrix}\n",
    "    0.5488135 & 0.71518937 & 0.60276338\\\\\n",
    "    0.54488318 & 0.4236548 & 0.64589411\\\\\n",
    "    0.43758721 & 0.891773 & 0.96366276\n",
    "    \\end{bmatrix}$\n",
    "​\n",
    "\n",
    " $W_2=\\begin{bmatrix}\n",
    "    0.56804456 & 0.92559664 & 0.07103606\\\\\n",
    "    0.0871293 & 0.0202184 & 0.83261985\\\\\n",
    "    0.77815675 & 0.87001215 & 0.97861834\n",
    "    \\end{bmatrix}$\n",
    "\n",
    "\n",
    "$W_3=\\begin{bmatrix}\n",
    "    0.11827443 & 0.63992102 & 0.14335329\\\\\n",
    "    0.94466892 & 0.52184832 & 0.41466194\\\\\n",
    "    0.26455561 & 0.77423369 & 0.45615033\n",
    "    \\end{bmatrix}$\n",
    "\n",
    "\n",
    "$b_1=\\begin{bmatrix}\n",
    "    0.38344152\\\\\n",
    "    0.79172504\\\\\n",
    "    0.52889492\n",
    "    \\end{bmatrix}$\n",
    "\n",
    "\n",
    "$b_2=\\begin{bmatrix}\n",
    "    0.79915856\\\\\n",
    "    0.46147936\\\\\n",
    "    0.78052918\n",
    "    \\end{bmatrix}$\n",
    "\n",
    "\n",
    "$b_3=\\begin{bmatrix}\n",
    "    0.56843395\\\\\n",
    "    0.0187898\\\\\n",
    "    0.6176355\n",
    "    \\end{bmatrix}$\n",
    "\n",
    "\n",
    "The weights that connects outputs from neurons in the previous $(i−1)$ layer to a neuron in the present $i^{th}$ layer correspond to a row in the weight matrix.The input to the network\n",
    "\n",
    "$x=\\begin{bmatrix}\n",
    "    1\\\\\n",
    "    0\\\\\n",
    "    1\n",
    "    \\end{bmatrix}$\n",
    "\n",
    "and the corresponding label\n",
    "\n",
    "$y=\\begin{bmatrix}\n",
    "    0\\\\\n",
    "    0\\\\\n",
    "    1\n",
    "    \\end{bmatrix}$\n",
    "\n",
    "\n",
    "All the neurons in the hidden layers use Sigmoid activation function and the neurons in the output layer uses Softmax function. Assume that the network uses the cross entropy loss (use natural log).\n",
    "\n",
    "You are advised to use the Numpy package to compute matrix vector multiplications. You can download the initial weights [HERE](https://drive.google.com/file/d/1TJax-Eq1I-TD3Fofsu4ODPLwCPsz1250/view)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = np.load('parameters.npz')\n",
    "W1=params.get('W1')\n",
    "W2=params.get('W2')\n",
    "W3=params.get('W3')\n",
    "b1=params.get('b1')\n",
    "b2=params.get('b2')\n",
    "b3=params.get('b3')\n",
    "x=np.array([1,0,1]).reshape(-1,1)\n",
    "y=np.array([0,0,1]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_activation(W, x, b):\n",
    "    return np.dot(W,x)+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z)/np.sum(np.exp(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropyloss(y, y_pred):\n",
    "    return -np.sum(y*np.log(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(W1, W2, W3, b1, b2, b3, x):\n",
    "\n",
    "    a1=pre_activation(W1,x,b1)\n",
    "    h1=sigmoid(a1)\n",
    "\n",
    "    a2=pre_activation(W2,h1,b2)\n",
    "    h2=sigmoid(a2)\n",
    "\n",
    "    a3=pre_activation(W3,h2,b3)\n",
    "    y_pred=softmax(a3)\n",
    "\n",
    "    return a1, h1, a2, h2, a3, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(y, y_pred, a3, h2, a2, h1, a1, x, W1, W2, W3):\n",
    "    del_y_pred = -y/y_pred\n",
    "\n",
    "    del_a3 = -(y-y_pred)\n",
    "    del_h2 = W3.T@del_a3\n",
    "    del_W3 = del_a3@h2.T\n",
    "    del_b3 = del_a3\n",
    "\n",
    "    del_a2 = del_h2*sigmoid(a2)*(1-sigmoid(a2))\n",
    "    del_h1 = W2.T@del_a2\n",
    "    del_W2 = del_a2@h1.T\n",
    "    del_b2 = del_a2\n",
    "\n",
    "    del_a1 = del_h1*sigmoid(h1)*(1-sigmoid(h1))\n",
    "    del_W1 = del_a1@x.T\n",
    "    del_b1 = del_a1\n",
    "    \n",
    "    return del_y_pred, del_a3, del_h2, del_a2, del_h1, del_a1, del_W3, del_W2, del_W1, del_b3, del_b2, del_b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1, h1, a2, h2, a3, y_pred = forward_prop(W1, W2, W3, b1, b2, b3, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "del_y_pred, del_a3, del_h2, del_a2, del_h1, del_a1, del_W3, del_W2, del_W1, del_b3, del_b2, del_b1 = back_prop(y, y_pred, a3, h2, a2, h1, a1, x, W1, W2, W3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. How many (learnable) parameters are there in the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m 36\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1;32m {}\".format(W1.shape[0]*W1.shape[1]+W2.shape[0]*W2.shape[1]+W3.shape[0]*W3.shape[1]+b1.shape[0]+b2.shape[0]+b3.shape[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is the sum of the elments of output $a_1$? (Choose the nearest option to your answer)\n",
    "\n",
    "(a) 3.7\n",
    "\n",
    "(b) 5.44\n",
    "\n",
    "(c) 4.16\n",
    "\n",
    "(d) 17.59"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m 5.45\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1;32m {:.2f}\".format(a1.sum()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. What is the sum of the elments of output $h_1$? (Choose the nearest option to your answer)\n",
    "\n",
    "(a) 2.57\n",
    "\n",
    "(b) 3.46\n",
    "\n",
    "(c) 0.67\n",
    "\n",
    "(d) 0.42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m 2.57\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1;32m {:.2f}\".format(h1.sum()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4. The sum of the elements of $[a_2, h_2, a_3]$ respectively are $[6.4, 2.63, 4.87]$. What is the loss value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.460166773282593, 2.63139309587371, 4.874920988265704)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2.sum(), h2.sum(), a3.sum() # checking the sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m 0.86\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1;32m {:.2f}\".format(cross_entropyloss(y, y_pred)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Choose the vector that corresponds to $\\nabla_{a_3}L(θ)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m [[ 0.23691422]\n",
      " [ 0.33838847]\n",
      " [-0.57530268]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1;32m {}\".format(del_a3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. We know that after computing gradients, we update the values of $b_2$ by subtracting its gradient ,i.e.,\n",
    "\n",
    "$b_2-\\eta\\nabla_{b_2}L(θ)$.\n",
    "\n",
    "Which of the following is the gradient vector of $b_2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m [[ 0.01838198]\n",
      " [-0.01997644]\n",
      " [-0.0038401 ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\033[1;32m {}\".format(del_b2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7. Update all the parameters with the calculated gradients. Forward propagate the input through the network. What is the new loss value ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;32m 0.07\n"
     ]
    }
   ],
   "source": [
    "W3-=del_W3\n",
    "b3-=del_b3\n",
    "W2-=del_W2\n",
    "b2-=del_b2\n",
    "W1-=del_W1\n",
    "b1-=del_b1\n",
    "\n",
    "a1, h1, a2, h2, a3, y_pred = forward_prop(W1, W2, W3, b1, b2, b3, x)\n",
    "\n",
    "print(\"\\033[1;32m {:.2f}\".format(cross_entropyloss(y, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
